
# remarks from train method - removed in 11/04/20

    # Load best model and continue learning
    # models = os.listdir(dir + '/model_dir/sac')
    # models_rew = (model for model in models if 'rew' in model)
    # ind, reward = [], []
    # for model in models_rew:
    #     ind.append(model.split('_')[1])
    #     reward.append(model.split('_')[3])
    # best_reward = max(reward)
    # best_model_ind = reward.index(best_reward)
    # k = ind[best_model_ind]
    # model = SAC.load(dir + '/model_dir/sac/test_' + k + '_rew_' + best_reward, env=env,
    #                  custom_objects=dict(learning_starts=0))
    # Load last saved model and continue learning
    # models = os.listdir(dir + '/model_dir/sac')
    # models_time = (model for model in models if 'rew' not in model)
    # ind, hour, min = [], [], []
    # for model in models_time:
    #     ind.append(model.split('_')[1])
    #     hour.append(model.split('_')[3])
    #     min.append(model.split('_')[4])
    # date = models_time[0].split('_')[2]
    # latest_hour = max(hour)
    # latest_hour_ind = [i for i, n in enumerate(hour) if n == latest_hour]
    # latest_min = max(min[latest_hour_ind])
    # latest_min_ind = min(latest_min)
    # k = ind[latest_min_ind]
    # model = SAC.load(dir + '/model_dir/sac/test_' + k + '_' + date + '_' + latest_hour[0] + '_' + latest_min + 'zip',
    #                  env=env, custom_objects=dict(learning_starts=0))
    # model = SAC.load(dir + '/model_dir/sac/test_53_rew_24383.0',
    #                  env=env, tensorboard_log=log_dir,
    #                  custom_objects=dict(learning_starts=0, learning_rate=2e-4,
    #                                      train_freq=8, gradient_steps=4, target_update_interval=4))
    # #                                              # batch_size=32))
    # Test the pre-trained model
    # env = model.get_env()
    # obs = env.reset()
    #
    # reward_sum = 0.0
    # for _ in range(1000):
    #     action, _ = model.predict(obs)
    #     obs, reward, done, _ = env.step(action)
    #     reward_sum += reward
    #     if done:
    #         print(reward_sum)
    #         reward_sum = 0.0
    #         obs = env.reset()
    #
    # env.close()

# The following template is taken from stable-baselines tutorial - removed in 11/04/20

    # class CustomCallback(BaseCallback):
    #     """
    #     A custom callback that derives from ``BaseCallback``.
    #
    #     :param verbose: (int) Verbosity level 0: not output 1: info 2: debug
    #     """
    #     def __init__(self, verbose=0):
    #         super(CustomCallback, self).__init__(verbose)
    #         # Those variables will be accessible in the callback
    #         # (they are defined in the base class)
    #         # The RL model
    #         # self.model = None  # type: BaseRLModel
    #         # An alias for self.model.get_env(), the environment used for training
    #         # self.training_env = None  # type: Union[gym.Env, VecEnv, None]
    #         # Number of time the callback was called
    #         # self.n_calls = 0  # type: int
    #         # self.num_timesteps = 0  # type: int
    #         # local and global variables
    #         # self.locals = None  # type: Dict[str, Any]
    #         # self.globals = None  # type: Dict[str, Any]
    #         # The logger object, used to report things in the terminal
    #         # self.logger = None  # type: logger.Logger
    #         # # Sometimes, for event callback, it is useful
    #         # # to have access to the parent object
    #         # self.parent = None  # type: Optional[BaseCallback]
    #
    #     def _on_training_start(self) -> None:
    #         """
    #         This method is called before the first rollout starts.
    #         """
    #         pass
    #
    #     def _on_rollout_start(self) -> None:
    #         """
    #         A rollout is the collection of environment interaction
    #         using the current policy.
    #         This event is triggered before collecting new samples.
    #         """
    #         pass
    #
    #     def _on_step(self) -> bool:
    #         """
    #         This method will be called by the model after each call to `env.step()`.
    #
    #         For child callback (of an `EventCallback`), this will be called
    #         when the event is triggered.
    #
    #         :return: (bool) If the callback returns False, training is aborted early.
    #         """
    #         return True
    #
    #     def _on_rollout_end(self) -> None:
    #         """
    #         This event is triggered before updating the policy.
    #         """
    #         pass
    #
    #     def _on_training_end(self) -> None:
    #         """
    #         This event is triggered before exiting the `learn()` method.
    #         """
    #         pass
